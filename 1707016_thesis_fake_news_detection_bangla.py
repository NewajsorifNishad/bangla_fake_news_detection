# -*- coding: utf-8 -*-
"""1707016_thesis_fake_news_detection_bangla.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mAddEM-bDVWqAXoP72rzmdDBLVCbta1x
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
import re
import string
import re
import pickle
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

df_real=pd.read_csv(r'/content/drive/MyDrive/myrealnews_last_final_4.csv',encoding='utf-8')
df_real.tail()

df_real.shape

df2=df_real.dropna()
df2=df_real.dropna(axis=0)
df2=df_real.dropna().reset_index(drop=True)

df2.shape

df2 = df2.drop(["label"], axis=1)

df2["label"]=1

df2["Article"]=df2["Article"].astype(str)

df2.dtypes

df_real=df2

df_real.iloc[0][7]

df_fake=pd.read_csv(r'/content/drive/MyDrive/myfakenews_final_1k_2.csv',encoding='utf-8')
df_fake.head()

df_fake.dtypes

df3=df_fake.dropna()
df3=df_fake.dropna(axis=0)
df3=df_fake.dropna().reset_index(drop=True)

df3.shape

df3["Article"]=df3["Article"].astype(str)

print(df_fake.loc[1031][7])

df_fake=df3

df_fake.dtypes

data1 =pd.read_excel(r'/content/drive/MyDrive/stop_word.xlsx')
stop = data1['word'].tolist()

class preprocess:
  '''def text_to_word_list(self,text):
    text = text.split()
    return text'''

  def replace_strings(self,text):
      emoji_pattern = re.compile("["
                            u"\U0001F600-\U0001F64F"  # emoticons
                            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                            u"\U0001F680-\U0001F6FF"  # transport & map symbols
                            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                            u"\U00002702-\U000027B0"
                            u"\U000024C2-\U0001F251"
                            u"\u00C0-\u017F"          #latin
                            u"\u2000-\u206F"          #generalPunctuations
                               
                             "]+", flags=re.UNICODE)
      english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)
     #latin_pattern=re.compile('[A-Za-z\u00C0-\u00D6\u00D8-\u00f6\u00f8-\u00ff\s]*',)
    
      text=emoji_pattern.sub(r'', text)
      text=english_pattern.sub(r'', text)
      return text
  def remove_punctuations(self,my_str):

      # define punctuation
     punctuations = '''````£|¢|Ñ+-*/=EROero৳012–34567•89।!()-[]{};:'"“\’,<>./?@#$%^&*_~‘—॥”‰🤣⚽️✌�￰৷￰'''
     no_punct = ""
     dari='''।''' 
     for char in my_str:
          if char not in punctuations:
             no_punct = no_punct + char

     # display the unpunctuated string
     return no_punct



  def stopwordRemoval(self,text): 

    x=str(text)
    l=x.split()

    stm=[elem for elem in l if elem not in stop]
    
    out=' '.join(stm)
    
    return str(out)
  def preprocessing(self,text):
    pat='।' 
    text=re.sub(pat,"  ",text)
    pat='-' 
    text=re.sub(pat,"  ",text)
    pat=',' 
    text=re.sub(pat,"  ",text)
    out=self.remove_punctuations(self.replace_strings(text))
    out2=self.stopwordRemoval(out)
    return out2

from collections import Counter
count= Counter()
class correct_chk():
  def __init__(self):
    self.count=Counter()
    with open("/content/drive/MyDrive/unique_vocab.txt") as fp:
      data = fp.read()
      for word in data.split():
        self.count[word]+=1
  def chk(self,value):
      if(self.count[value]==0):
           return 0

prep=preprocess()
chk=correct_chk()

df_real['headline'] = df_real.headline.apply(lambda x: prep.preprocessing(str(x)))
df_real['Article'] = df_real.Article.apply(lambda x: prep.preprocessing(str(x)))
df_real.head()

print(df_real.loc[9][7])

df_fake['headline'] = df_fake.headline.apply(lambda x: prep.preprocessing(str(x)))
df_fake['Article'] = df_fake.Article.apply(lambda x: prep.preprocessing(str(x)))
df_fake.head()

print(df_fake.loc[1031][7])

cnt = Counter()
cntf = Counter()

for text in df_real['Article'].values:
  for word in text.split():
    cnt[word]+=1
#cnt.most_common(50)

for text in df_fake['Article'].values:
  for word in text.split():
    cntf[word]+=1
#cntf.most_common(50)

df_fake.shape, df_real.shape

df_marge = pd.concat([df_fake, df_real], axis =0 )
df_marge.head(10)

df_marge.columns

df_h = df_marge.drop(["Id","Domain", "publish_date","Category","Source","relation"],axis = 1)

df_h.isnull().sum()

df_h = df_h.sample(frac = 1)

df_h.head()

df_h.reset_index(inplace = True)
df_h.drop(["index"], axis = 1, inplace = True)

df_h.columns

df_h.head()

x_h = df_h["headline"]
y_h = df_h["label"]
x = df_h["Article"]

x_train_h, x_test_h, y_train_h, y_test_h = train_test_split(x_h, y_h, test_size=0.25,random_state=42)

x_train, x_test, y_train, y_test= train_test_split(x, y_h, test_size=0.25,random_state=42)

x_train_h.head()

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
cv_h = CountVectorizer(tokenizer=lambda x: x.split())
xv_train_h = cv_h.fit_transform(x_train_h) # cv embed for headline
xv_test_h = cv_h.transform(x_test_h)
print(xv_test_h.shape)

print(xv_train_h.shape)

xv_train_h

cv = CountVectorizer(tokenizer=lambda x: x.split())
xv_train = cv.fit_transform(x_train)  #cv embed for article
xv_test = cv.transform(x_test)
print(xv_train.shape)

xv_train

xv_train_array=xv_train.toarray()
print(xv_train_array[0])

xv_train_h_array=xv_train_h.toarray()
print(xv_train_h_array)

xv_train_h

xv_train_h_array=xv_train_h.toarray()
for r in range(8580):
  if(xv_train_h_array[1][r]>0.0):
    print(xv_train_h_array[1][r])
    print(r)

freqs = zip(cv.get_feature_names_out(), np.asarray(xv_train.sum(axis=0)).ravel())
print(sorted(freqs, key=lambda x: -x[1]))
#cv.get_feature_names()
#cv.vocabulary_

print(xv_train_h_array[0].shape)

LR_cv_h = LogisticRegression(solver='lbfgs', max_iter=20000, random_state=3) # cv LR train and test for headline only 
LR_cv_h.fit(xv_train_h,y_train_h)
predLR_cv_h=LR_cv_h.predict(xv_test_h)
LR_cv_h.score(xv_test_h, y_test_h)
print(classification_report(y_test_h, predLR_cv_h))

LR_cv_a = LogisticRegression(solver='lbfgs', max_iter=20000, random_state=3) # cv LR train and test for Article only 
LR_cv_a.fit(xv_train,y_train)
predLR_cv_a=LR_cv_a.predict(xv_test)
LR_cv_a.score(xv_test, y_test)
print(classification_report(y_test, predLR_cv_a))

def output_lable(n):      # for LR_CV both head and article
    if n == 0:
        return n
    elif n == 1:
        return n
    
def manual_testing_both_lr_cv(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]
  '''for value in new_x_test.split(" "):
    y=y+1
    if (chk.chk(value)==0):
        m=m+1
        print("Spelling error: " + value)
  error=m/y'''
  if(0==1):
    return "LR_CV_prediction:Fake News"
  else:
          new_xv_test = cv.transform(new_def_test["text"])
          pred_a =LR_cv_a.predict(new_xv_test)
          new_xv_test_h = cv_h.transform(new_def_test_h["text"])
          pred_h = LR_cv_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(a), 
                                                                      #))

p=0              # for LR_cv both head and article
tn=0
fn=0
tp=0
fp=0
for r in range(1066):
  p=manual_testing_both_lr_cv(x_test_h.iloc[r],x_test.iloc[r])
  if(p==0):
    if(y_test_h.iloc[r]==0):
       tn+=1
    else:
       fn+=1
  else:
    if(y_test_h.iloc[r]==1):
       tp+=1
    else:
       fp+=1

print(tp)
print(fp)
print(tn)
print(fn)

lr_cv=[[tp,fp],
       [fn,tn]]
ax = sns.heatmap(lr_cv, annot=True, cmap='Blues', fmt='g')

ax.set_title('Logistic Regression with CountVectorizer:\n\n');
ax.set_xlabel('\nActual Values')
ax.set_ylabel('Predicted Value ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['True','False'])
ax.yaxis.set_ticklabels(['True','False'])

## Display the visualization of the Confusion Matrix.
plt.show()

Accuracy= (tp+tn)/(tp+fn+tn+fp)
precision= (tp)/(tp+fp)
Recall= (tp)/(tp+fn)
F1= (2*precision*Recall)/(precision+Recall)

print("Accuracy=",Accuracy)
print("Precision=",precision)
print("Recall=",Recall)
print("F1=",F1)

pa_cv_h=PassiveAggressiveClassifier(max_iter=5000, random_state=42)   #cv pac train and test only for headline
pa_cv_h.fit(xv_train_h,y_train_h)
predpa_cv_h=pa_cv_h.predict(xv_test_h)
pa_cv_h.score(xv_test_h, y_test_h)
print(classification_report(y_test_h,predpa_cv_h))

pa_cv_a=PassiveAggressiveClassifier(max_iter=5000,random_state=42)   #cv pac train and test only for article
pa_cv_a.fit(xv_train,y_train)
predpa_cv_a=pa_cv_a.predict(xv_test)
pa_cv_a.score(xv_test, y_test)
print(classification_report(y_test,predpa_cv_a))

def output_lable(n):      # for pac_CV both head and article
    if n == 0:
        return n
    elif n == 1:
        return n
    
def manual_testing_both_pa_cv(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]
  '''for value in new_x_test.split(" "):
    y=y+1
    if (chk.chk(value)==0):
        m=m+1
        print("Spelling error: " + value)
  error=m/y'''
  if(0==1):
    return print("\n\nLR Prediction: {}".format(output_lable(0), ))
  else:
          new_xv_test = cv.transform(new_def_test["text"])
          pred_a =pa_cv_a.predict(new_xv_test)
          new_xv_test_h = cv_h.transform(new_def_test_h["text"])
          pred_h = pa_cv_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(a), 
                                                                      #))

p=0              # for pa_cv both head and article
tn=0
fn=0
tp=0
fp=0
for r in range(1066):
  p=manual_testing_both_pa_cv(x_test_h.iloc[r],x_test.iloc[r])
  if(p==0):
    if(y_test_h.iloc[r]==0):
       tn+=1
    else:
       fn+=1
  else:
    if(y_test_h.iloc[r]==1):
       tp+=1
    else:
       fp+=1

print(tp)
print(fp)
print(tn)
print(fn)

lr_cv=[[tp,fp],
       [fn,tn]]
ax = sns.heatmap(lr_cv, annot=True, cmap='Blues', fmt='g')

ax.set_title('Passive Aggressive classifier with CountVectorizer:\n\n');
ax.set_xlabel('\nActual Values')
ax.set_ylabel('Predicted Value ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['True','False'])
ax.yaxis.set_ticklabels(['True','False'])

## Display the visualization of the Confusion Matrix.
plt.show()

Accuracy= (tp+tn)/(tp+fn+tn+fp)
precision= (tp)/(tp+fp)
Recall= (tp)/(tp+fn)
F1= (2*precision*Recall)/(precision+Recall)

print("Accuracy=",Accuracy)
print("Precision=",precision)
print("Recall=",Recall)
print("F1=",F1)

vectorization_h = TfidfVectorizer(tokenizer=lambda x: x.split())     # TF embed for headline only 
xv_train_h = vectorization_h.fit_transform(x_train_h)
xv_test_h = vectorization_h.transform(x_test_h)

vectorization = TfidfVectorizer(tokenizer=lambda x: x.split())     # TF embed for article only 
xv_train = vectorization.fit_transform(x_train)
xv_test = vectorization.transform(x_test)

xv_train

'''xv_train_h_array=xv_train_h.toarray()
for r in range(8621):
  if(xv_train_h_array[1][r]>0.0):
    print(xv_train_h_array[1][r])
    print(r)'''

xv_train_h
xv_train_array_2=xv_train_h.toarray()
#vectorization.vocabulary_

print(xv_train_array_2)

xv_train

LR_tf_h = LogisticRegression( random_state=0)  # Tf LR train and test only for headline 
LR_tf_h.fit(xv_train_h,y_train_h)
predLR_tf_h=LR_tf_h.predict(xv_test_h)
LR_tf_h.score(xv_test_h, y_test_h)
print(classification_report(y_test_h,predLR_tf_h))

LR_tf_a = LogisticRegression(random_state=0)  # Tf LR train and test only for article
LR_tf_a.fit(xv_train,y_train)
predLR_tf_a=LR_tf_a.predict(xv_test)
LR_tf_a.score(xv_test, y_test)
print(classification_report(y_test,predLR_tf_a))

pa_tf_h=PassiveAggressiveClassifier(max_iter=5000,random_state=42)   #Tf pac train and test only for headline
pa_tf_h.fit(xv_train_h,y_train_h)
predpa_tf_h=pa_tf_h.predict(xv_test_h)
pa_tf_h.score(xv_test_h, y_test_h)
print(classification_report(y_test_h,predpa_tf_h))

pa_tf_a=PassiveAggressiveClassifier(max_iter=5000,random_state=42)   #Tf pac train and test only for article
pa_tf_a.fit(xv_train,y_train) 
predpa_tf_a=pa_tf_a.predict(xv_test)
pa_tf_a.score(xv_test, y_test)
print(classification_report(y_test,predpa_tf_a))

def output_lable(n):      # for LR_TF both head and article
    if n == 0:
        return n
    elif n == 1:
        return n
    
def manual_testing_both_lr_tf(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]
  '''for value in new_x_test.split(" "):
    y=y+1
    if (chk.chk(value)==0):
        m=m+1
        print("Spelling error: " + value)
  error=m/y'''
  if(0==1):
    return print("\n\nLR Prediction: {}".format(output_lable(0), ))
  else:
          new_xv_test = vectorization.transform(new_def_test["text"])
          pred_a = LR_tf_a.predict(new_xv_test)
          new_xv_test_h = vectorization_h.transform(new_def_test_h["text"])
          pred_h = LR_tf_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(a), 
                                                                      #))

p=0              # for LR_TF both head and article
tn=0
fn=0
tp=0
fp=0
for r in range(1066):
  p=manual_testing_both_lr_tf(x_test_h.iloc[r],x_test.iloc[r])
  if(p==0):
    if(y_test_h.iloc[r]==0):
       tn+=1
    else:
       fn+=1
  else:
    if(y_test_h.iloc[r]==1):
       tp+=1
    else:
       fp+=1

print(tp)
print(fp)
print(tn)
print(fn)

lr_cv=[[tp,fp],
       [fn,tn]]
ax = sns.heatmap(lr_cv, annot=True, cmap='Blues', fmt='g')

ax.set_title('Logistic Regression with Tf-idf:\n\n');
ax.set_xlabel('\nActual Values')
ax.set_ylabel('Predicted Value ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['True','False'])
ax.yaxis.set_ticklabels(['True','False'])

## Display the visualization of the Confusion Matrix.
plt.show()

Accuracy= (tp+tn)/(tp+fn+tn+fp)
precision= (tp)/(tp+fp)
Recall= (tp)/(tp+fn)
F1= (2*precision*Recall)/(precision+Recall)

print("Accuracy=",Accuracy)
print("Precision=",precision)
print("Recall=",Recall)
print("F1=",F1)

def output_lable(n):    # for pac both head and article
    if n == 0:
        return n
    elif n == 1:
        return n
    
def manual_testing_both_pa_tf(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]
  '''for value in new_x_test.split(" "):
    y=y+1
    if (chk.chk(value)==0):
        m=m+1
        print("Spelling error: " + value)
  error=m/y'''
  if(0==1):
    return print("\n\nLR Prediction: {}".format(output_lable(0), ))
  else:
          new_xv_test = vectorization.transform(new_def_test["text"])
          pred_a = pa_tf_a.predict(new_xv_test)
          new_xv_test_h = vectorization_h.transform(new_def_test_h["text"])
          pred_h = pa_tf_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(a), 
                                                                      #))

p=0              # for pa_TF both head and article
tn=0
fn=0
tp=0
fp=0
for r in range(1066):
  p=manual_testing_both_pa_tf(x_test_h.iloc[r],x_test.iloc[r])
  if(p==0):
    if(y_test_h.iloc[r]==0):
       tn+=1
    else:
       fn+=1
  else:
    if(y_test_h.iloc[r]==1):
       tp+=1
    else:
       fp+=1

print(tp)
print(fp)
print(tn)
print(fn)

lr_cv=[[tp,fp],
       [fn,tn]]
ax = sns.heatmap(lr_cv, annot=True, cmap='Blues', fmt='g')

ax.set_title('Passive Aggressive Classifier with CountVectorizer:\n\n');
ax.set_xlabel('\nActual Values')
ax.set_ylabel('Predicted Value ');

## Ticket labels - List must be in alphabetical order
ax.xaxis.set_ticklabels(['True','False'])
ax.yaxis.set_ticklabels(['True','False'])

## Display the visualization of the Confusion Matrix.
plt.show()

Accuracy= (tp+tn)/(tp+fn+tn+fp)
precision= (tp)/(tp+fp)
Recall= (tp)/(tp+fn)
F1= (2*precision*Recall)/(precision+Recall)

print("Accuracy=",Accuracy)
print("Precision=",precision)
print("Recall=",Recall)
print("F1=",F1)

def output_lable(n):    # User input for both LR_TF 
    if n == 0:
        return "Fake News"
    elif n == 1:
        return "Not A Fake News"
    
def manual_testing_both_LR_tf_u(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]

  for value in new_x_test.split(" "):
    y=y+1
    #if not cor.is_banan_correctly(value):
    if (chk.chk(value)==0):
        m=m+1
        #print("Spelling error: " + value)
  error=m/y
  if(error>0.2):
    print("hh")
    return 0
  else:
    Search_String=['বলেন','বলেছেন','জানান','জানিয়েছেন','জানিয়েছে','মন্তব্য করে','মন্তব্য করেছেন','তথ্য জানায়','তথ্য দিয়েছে','জানা যায়','জানায়','জানা গেছে','জানিয়েছে','শুনানি','বিজ্ঞপ্তিতে','আদালত','সূত্র','তথ্য নিশ্চিত','ঘোষণা','আবেদন','প্রতিবেদনে']
    for j in range(20): 
       pat=re.search(Search_String[j], new_x_test)
       if(pat!=None):
         k=0
       else:
        m=m+1
    if(m==20):
          return 0
    else:
          new_xv_test = vectorization.transform(new_def_test["text"])
          pred_a = LR_tf_a.predict(new_xv_test)
          #pred_a[0]=0
          new_xv_test_h = vectorization_h.transform(new_def_test_h["text"])
          pred_h = LR_tf_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(pred_LR_h[0]), 
                                                                    #  ))

def output_lable(n):    # User input for both LR_CV
    if n == 0:
        return "Fake News"
    elif n == 1:
        return "Not A Fake News"
    
def manual_testing_both_LR_cv_u(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]

  for value in new_x_test.split(" "):
    y=y+1
    #if not cor.is_banan_correctly(value):
    if (chk.chk(value)==0):
        m=m+1
        #print("Spelling error: " + value)
  error=m/y
  if(error>0.2):
    print("hh")
    return 0
  else:
    Search_String=['বলেন','বলেছেন','জানান','জানিয়েছেন','জানিয়েছে','মন্তব্য করে','মন্তব্য করেছেন','তথ্য জানায়','তথ্য দিয়েছে','জানা যায়','জানায়','জানা গেছে','জানিয়েছে','শুনানি','বিজ্ঞপ্তিতে','আদালত','সূত্র','তথ্য নিশ্চিত','ঘোষণা','আবেদন','প্রতিবেদনে','জরিপে']
    for j in range(21): 
       pat=re.search(Search_String[j], new_x_test)
       if(pat!=None):
         k=0
       else:
        m=m+1
    if(m==20):
          return 0
    else:
          new_xv_test = cv.transform(new_def_test["text"])
          pred_a = LR_cv_a.predict(new_xv_test)
          new_xv_test_h = cv_h.transform(new_def_test_h["text"])
          pred_h = LR_cv_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(pred_LR_h[0]), 
                                                                    #  ))

def output_lable(n):       # user input for both PA_TF
    if n == 0:
        return "Fake News"
    elif n == 1: 
        return "Not A Fake News"
    
def manual_testing_both_pa_tf_u(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]

  for value in new_x_test.split(" "):
    y=y+1
    #if not cor.is_banan_correctly(value):
    if (chk.chk(value)==0):
        m=m+1
        #print("Spelling error: " + value)
  error=m/y
  if(error>0.2):
    print("hh")
    return 0
  else:
    Search_String=['বলেন','বলেছেন','জরিপে','জানান','জানিয়েছেন','জানিয়েছে','মন্তব্য করে','মন্তব্য করেছেন','তথ্য জানায়','তথ্য দিয়েছে','জানা যায়','জানায়','জানা গেছে','জানিয়েছে','শুনানি','বিজ্ঞপ্তিতে','আদালত','সূত্র','তথ্য নিশ্চিত','ঘোষণা','আবেদন','প্রতিবেদনে']
    for j in range(21): 
       pat=re.search(Search_String[j], new_x_test)
       if(pat!=None):
         k=0
       else:
        m=m+1
    if(m==20):
          return 0
    else:
          new_xv_test = vectorization.transform(new_def_test["text"])
          pred_a = pa_tf_a.predict(new_xv_test)
          new_xv_test_h = vectorization_h.transform(new_def_test_h["text"])
          pred_h = pa_tf_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(pred_LR_h[0]), 
                                                                    #  ))

def output_lable(n):       # user input for both PA_cv
    if n == 0:
        return "Fake News"
    elif n == 1: 
        return "Not A Fake News"
    
def manual_testing_both_pa_cv_u(head,news):
  m=0
  y=0


  testing_news_h = {"text":[head]}  
  new_def_test_h = pd.DataFrame(testing_news_h)
  new_def_test_h["text"] = new_def_test_h.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test_h = new_def_test_h.iloc[0]["text"]

  testing_news = {"text":[news]}
  new_def_test = pd.DataFrame(testing_news)
  new_def_test["text"] = new_def_test.text.apply(lambda x: prep.preprocessing(str(x)))
  new_x_test = new_def_test.iloc[0]["text"]

  for value in new_x_test.split(" "):
    y=y+1
    #if not cor.is_banan_correctly(value):
    if (chk.chk(value)==0):
        m=m+1
        #print("Spelling error: " + value)
  error=m/y
  if(error>0.2):
    print("hh")
    return 0
  else:
    Search_String=['বলেন','বলেছেন','জানান','জানিয়েছেন','জানিয়েছে','মন্তব্য করে','মন্তব্য করেছেন','তথ্য জানায়','তথ্য দিয়েছে','জানা যায়','জানায়','জানা গেছে','জানিয়েছে','শুনানি','বিজ্ঞপ্তিতে','আদালত','সূত্র','তথ্য নিশ্চিত','ঘোষণা','আবেদন','প্রতিবেদনে']
    for j in range(20): 
       pat=re.search(Search_String[j], new_x_test)
       if(pat!=None):
         k=0
       else:
        m=m+1
    if(m==20):
          return 0
    else:
          new_xv_test = cv.transform(new_def_test["text"])
          pred_a = pa_cv_a.predict(new_xv_test)
          new_xv_test_h = cv_h.transform(new_def_test_h["text"])
          pred_h = pa_cv_h.predict(new_xv_test_h)      
          if(pred_a[0]==1 and pred_h[0]==1):
            return 1
          else:
            return 0
          #return print("\n\nLR Prediction: {}".format(output_lable(pred_LR_h[0]), 
                                                                    #  ))

def all_manual(headlin,article):
  p=manual_testing_both_LR_cv_u(headlin,article) 
  if(p==0):
    lrcv= "Logistic Regression with CV prediction: Fake news"
  else:
    lrcv= "Logistic Regression with CV prediction: True News"
  p=manual_testing_both_pa_cv_u(headlin,article) 
  if(p==0):
    pacv= "Passive aggressive with CV prediction: Fake news"
  else:
    pacv= "Passive aggressive with CV prediction: True News"
  p=manual_testing_both_pa_tf_u(headlin,article) 
  if(p==0):
    patf= "Passive aggressive with tf-idf prediction: Fake news"
  else:
    patf= "Passive aggressive with tf-idf prediction: True News"
  p=manual_testing_both_LR_tf_u(headlin,article) 
  if(p==0):
    lrtf= "Logistic Regression with tf-idf prediction: Fake news"
  else:
    lrtf= "Logistic Regression with tf-idf prediction: True News"
  return(lrcv,pacv,patf,lrtf)

pip install gradio

import gradio as gr

iface=gr.Interface(fn=all_manual,inputs=["text","text"],outputs=["text","text","text","text"])
iface.launch(share=True)